Intro
-----

*[slide: Scenario-Driven Development]

I'm going to talk today about Scenario-Driven
Development. Scenario-Driven Development is an extension of
Test-Driven Development to include the entire application, rather than
just individual units of code, and the process of the entire team,
rather than just developers' story implementation.

I'm going to talk at length about the practice of implementing
Scenario-Driven Development. Taken to the extremes that I advocate, it
is a rather ambitious change to the way many team develop software. I
hope to convince you that it is the correct approach, but I know that
I may not succeed. However there are two points that I want to get
make which are more generally applicable -- the Main Ideas that I
really want you to remember from this talk. I shall flag them clearly
as I go along.

*[slide: red/green/refactor]

My premise for this talk is that automated testing is good. Over the
last ten years there has been massive growth of developer automated
unit testing which has brought huge benefits to the entire development
process. The key to making the most of unit testing seems to be
allowing it to drive the writing of the code: TDD. Lots of teams that
I have been part of and heard of have tried to extend those benefits
to higher-level testing: to functional or acceptance tests (in my
experience terms for different types of testing are the source of much
disagreement; my solution is to use a different name altogether).

*[slide: happy face]

But those attempts have often been unsuccessful in the long term. My
aim here is to codify what it is that can make such an attempt
successful and how to avoid the pitfalls. The outcome of that
codification is Scenario-Driven Development.

*[slide: sad face]

Before we talk about how to do it, let's briefly touch on how not to
do it. The poster-child for unsuccessful approaches to automated
acceptance tests is the Story Test. In this approach each story that a
team delivers is accompanied by a one or more tests; the tests are
labelled as being associated with that story; each acceptance criteria
of the story is automated (to the extent that is practical). I have
never seen this approach succeed, although I have been desperate for
it to do so. Not many years ago I could be heard loudly proclaiming
the indispensability of such an approach on a large team which was
struggling to find a way of automating testing; fortunately few people
listened to me and the initiative died of natural causes.

*[slide: lots of tests]

The natural result of this approach is that you end up with a very
large number of tests. This large number has three problems. Each one
of the tests has to be written, it has to be run and it has to be
maintained.

The process of writing the tests always takes longer than you expect
it to; but this is the least of the problems. As the number of tests
increases, so does the run time; however hard to try to speed them up,
there are always more and more of them clogging up your build
pipeline. And every time you change the application a whole swathe of
the tests breaks. This is the real killer; however hard you try to
localize necessary changes by factoring out setup code, by cleanly
modelling the application in one place, they will fail
unexpectedly. And debugging a failing test for a story that someone else
implemented nine months ago, in a corner of the application that no one
cares about any more is truly heart-breaking. So we give up.

*[slide: few segregated tests]

The natural antidote to these problems is to cut your tests along a
different grain. Instead of aligning them with the historical accident
of how the application was developed, you align them, more naturally,
with the functionality of the application. You have a small suite of
tests for each area of functionality. This is an improvement: it
forces you to revisit the tests when a new story touches that area of
functionality, so you have an opportunity to refactor them; it
localizes breakages caused by changes to the way the application
works; it makes the tests more discoverable.

*[slide: many segregated tests]

But there are still problems. You are still likely to end up with a
lot of tests (with attendant speed problems). There is no obvious way
to decide where to test a new bit of functionality; decisions on this
are arbitrary and therefore likely to be inconsistent. People will
tend to just add a new test rather than modifying existing ones, with
a consequent slide back to one-test-per-story. And there is no
alignment of the tests with the team's (and the customer's)
understanding of the application.

The solution to all these problems seems to be two-fold: align the
tests not with functionality but with the activities that the
application is designed to support; and be brutal in leaving things
out of these tests, rather than trying to automate all acceptance criteria.

*[slide: people]

So we express the tests as a number of scenarios. Each one
encapsulates an activity that the application supports and asserts
that it really does support that activity. The tests are written in
the language of the user of the system, in terms of their interaction
with the application and their experience of using it. Scenarios
provide a solution to the problem of acceptance tests by boiling them
down to a simple description of the most important aspects of the
system, in a language that makes sense to the people who use it.

The technique of Scenario-Driven Development that I am advocating goes
beyond that, though. I believe that in the same way we use unit tests
in TDD to drive the design of the code, we should use scenarios to
frame the development of the whole application. They become a tool for
collaboration amongst the entire team and help shape a shared
understanding within the team of what it is they are developing.

In order to be successful in this role, the scenario tests have to
have several properties. Their technical properties address the
problems with automated testing that we discussed earlier: they need
to be fast and they need to be robust in the face of changes to the
application. More important, though, are their social properties.

First they need to be readable. I would hope that every member of the
team, including the customer, would be able to read the scenarios and
agree that they make sense. This doesn't necessarily preclude the
possibility of their being written in code: but if they are then the
code must be extremely expressive and the customers must be prepared
to put a bit of effort into learning to screen out the various braces
and semicolons that litter the text.

Second they must give the right impression. Reading the scenarios
should give a true impression of the intention of the
application. This means, more than anything, leaving some things out
that, while necessary, are not core to the activities that the
application is meant to support. The identity of the scenarios and the
workflow that they describe must be carefully chosen to give the right
impression without unnecessarily constraining the application or
cluttering them with unhelpful details.

Before I go any further, I want to acknowledge my sources.

*[slide: story mapping]

The idea of scenarios as a way of framing the entire development
process is closely related to story mapping; I recommend Jeff Patton's
writing on the subject.

*[slide: BDD]

Scenario-Driven Development is a close cousin of Behaviour-Driven
Development; many of my ideas are directly stolen from Dan North.

*[slide: GOOS]

The technical approach that I use for implementing scenarios owes a
lot to Nat Pryce and Steve Freeman's book which I wholeheartedly
recommend to all developers.

Example
-------

[slide: team]

*Project*

I'm going to talk through how Scenario-Driven Development works in
practice. I want to put that into a realistic context, so I'm going to
describe an example project and explain how the team uses scenarios to
frame their development process.

First, let me introduce you to the team.

[slide: Parvinder]

This is Parvinder, the project manager; he spends a lot of time
worrying and fiddling with spreadsheets -- we won't have much more to
do with him.

[slide: Bethany]

This is Bethany, the business analyst; she is responsible for
understanding the customers' needs and turning them into words of one
syllable that the rest of the team understand.

[slide: Terrence]

This is Terrence, the tester; he helps Bethany write the acceptance
criteria for the stories, helps the developers implement the stories
and finally checks that everything is working properly -- it rather
sounds like he does all the work.

[slide: developers]

And these are Doreen, Delilah and Dave; as you will have guessed, they
are the developers; they write code.

The team are using an XP-ish agile process.

[slide: Cesar]

They work closely with the customer (here's the customer, he's called
Cesar) to understand what the application needs to do.

[slide: story]

These requirements are expressed as stories, which have a narrative
and brief acceptance criteria.

[slide: story and team]

Much of the understanding of the stories is a shared, social
understanding amongst the members of the team rather than being
rigorously documented.

The whole team is involved, to some extent, throughout the development
process, collaborating and ensuring that everyone shares an
understanding of where the application is headed.

*[slide: money]

The application the team is working on is an online banking app. It
allows the bank's customers to see their statements, make payments,
all the usual sort of stuff.

*Scenarios*

[slide: scenario]

As well as unit tests, and the odd integration test to check the
interaction between their code and external systems, the team has a
set of scenario tests which describe the main activities that the
application supports. I'm going to show you a couple of their
scenarios.

[slide: team and scenario]

The ownership of these scenarios is shared amongst the team.  Bethany
(the BA) is responsible for what the scenarios should be, that is to
say for agreeing with Cesar (the customer) what activities the
application supports and how the application will be used.  Terrence
decides exactly what the workflow of each scenario is and owns their
textual description.  Doreen, Delilah and Dave write the code that
automate the scenarios.

This is the "Patricia Pays Her Phone Bill" scenario.

[slide: twist screenshot]

{ Patricia exists. }
{ Patricia has £22 in her current account. }
{ Patricia has at least £50 in her savings account. }
{ The system is running. }
  # Patricia goes to her home page.
  # She transfers £50 from her savings account to her current account.
  # She sees that her current account balance is £72.
  # She goes to the payments page.
  # She enters a payment of £48 to 12345678, sort code 12-12-12.
  # She submits the payment.
  # She goes to her home page.
  # She sees that her current account balance is £24.

It describes a customer, Patricia, transferring the money from a
savings account to her current account entering the payment details,
submitting it and finally going back to her home page to see that her
account has been debited. When this scenario is run it will, through
the magic of code, get the application into the right state for the
scenario, drive the application through the workflow and ensure that
it behaves as expected.

This context describes the necessary prerequisites for the scenario to
run: the application is running, Patricia is provisioned on the system
and has enough money in her account to make the payment successfully.

And here is another scenario: "Pierre Wonders Why He Is So
Poor".

[slide: twist screenshot] 

{ Pierre exists. }
{ Pierre has made 12 payments to Amazon. }
{ Pierre has made a £132 payment to Sevilla Mia. }
{ The system is running. }
# Pierre goes to his home page.
# Pierre looks at the statement for his current account.
# Pierre examines his outgoings.
# Pierre sees that his outgoings include a £132 payment to Sevilla
  Mia.
# Pierre sees that his outgoings include 12 payments to Amazon.
# Pierre goes to the loan application page.

Pierre looks at the statement for his account, filters it to
show out-goings only, gives a little moan of despair and then follows
a link to find out about low-rate loans.

[slide: scenarios]

There are several other scenarios, but not a huge number. Each one
describes how the users of the system will carry out some important
activity. The scenarios are aligned with those activities rather than
with the functionality that they exercise. For example there isn't a
"Transfers" scenario, although the ability to make transfers between
accounts is an important feature of the system. But the scenarios are
at a higher level than that -- they describe the motivations behind
the application as well as how it works.

[slide: bethany, cesar and scenarios]

The scenarios were roughly sketched out right at the beginning of the
project, by Bethany and Cesar, before they drilled down into the
features that would need to go into the application.

[slide: team and scenarios]

They've been used to guide the whole team since then: to help define
and prioritize features, to ensure the team shares an understanding of
what the application is for and, through their embodiment as automated
tests, to ensure that the application really does support the
activities it has been designed for.

*A story*

*[slide: mingle screenshot]

So what happens when the team plays a new story? Here is the story
they're working on now: "Stop account going overdrawn".

  In order that I can control who I make loans to,
  as a Bank Manager,
  I want payments which would cause accounts to go overdrawn to fail.

(You understand that this is pretty early in the development off the
application.)

So the system will support overdrafts, but they haven't been
implemented yet. For now it is just going to deny attempts to make an
account go overdrawn. Here are the acceptance criteria:

* A payment that would make the account go overdrawn fails.
* A message is displayed to the customer explaining what went wrong.
* The customer stays on the payment page with the fields of the form
  all filled in.
* No money is taken out of the account.
* A payment which leaves the account empty is successful.

*Updating the scenarios*

*[slide: terrence and scenario]

When this story is played, Terrence (the tester) needs to think about
how it affects the scenarios. In the "Patricia Pays her Phone Bill"
scenario, we saw Patricia transfer money into her current account in
order to make the payment. Here's an opportunity to exercise this new
functionality: Patricia can try to make the payment before there is
enough money in her current account to do so.

But the story does not affect on the "Pierre Wonders Why He Is So Poor"
scenario. (Although it does affect the "Pierre Tries To Save Up For A
Car" scenario. Poor Pierre.)

Note that we probably don't want to create a new failing payment
scenario. There are two good reasons for this. Failing to make a
payment probably isn't one of the activities that Cesar particularly
wants to support. This is not to say that the functionality to stop
people going overdrawn isn't important, it is, but isn't in itself a
motivating force behind the system.

There is another reason for keeping the number of scenarios small. In
order that they are realistic tests of the whole system, they may need
to be relatively slow. They may have to bring up servers, make network
or database calls and so on. It is important to keep the tests as fast
as is reasonably possible (and I'll talk about some techniques to help
with that later), but they can't be edge-to-edge system tests and
still remain as lightning fast as unit tests.

[slide: updated screenshot]

So Terrence updates "Patricia pays her Phone Bill" to reflect the new
functionality. A new section of the test sees Patricia trying to make
the payment before she has transferred the money from her savings
account. The payment fails and she sees a message explaining why.

Note that this updated scenario doesn't reflect all the acceptance
criteria on the story. This is one of the points where you have to be
careful when deciding what to put into the scenarios (and this is one
of the two Main Ideas that I want to pass on to you today -- that what
you leave out of your tests is as important as what you put in). We
don't test that no money has come out of the account; we don't check
that the fields are all still filled in (we also don't have a test for
the edge case of bringing the balance down to zero).

This is not to say that these acceptance criteria will not have
automated tests. They will, but those tests will be lower-level tests,
maybe unit tests, targeted at the components of the system responsible
for enforcing the behaviour. The main reason is that we don't want to
clutter up the scenarios with details and edge cases. We want them to
be a clear description of the intent of the system and the activities
it supports.

*[slide: team and scenario]

One thing people say about Test-Driven Development is that "it's not
about the tests", it's about the rigour of the process and the design
that the tests drive out. In the same way you could say that Scenario
Drive Development isn't about the tests, it's about the shared
understanding of the system that they bring to the team. Obviously the
automated tests that are a byproduct of both approaches are extremely
valuable and we wouldn't want to be without them.

*[slide: updated scenario]

Now one part of this scenario, where it checks for the message,
is completely new. The message wasn't there before, so the code
underlying the scenario won't yet support checking for its
presence. That code needs to be written.

I should point out that there is code underlying these scenarios. This
is a screenshot from Twist, a tool produced by ThoughtWorks Studios
which allows scenarios written in this way to be mapped directly onto
code. More excitingly it is actually a refactoring IDE for tests. This
scenario, and the underlying code, can be refactored just as you would
refactor normal code. I'll talk more about Twist later and show it to
you in action. I'll also talk about other technologies that can be
used.

So right now, in our story, this scenario *fails*. In fact it doesn't
even compile.

*[slide: developers and scenario]

To get it compiling, and eventually passing, we need to turn to
Doreen, Delilah and Dave.

*Implementing the story*

Before we talk about the process for implementing the story, We need
to stop and talk about how the test harness for the scenarios is
structured. The approach I'm going to describe is roughly technology
agnostic.

*[slide: architecture]

The top layer is the scenarios themselves. They sit on top of a
workflow layer which models the process of using the application. In
our scenario when we see Patricia going to the payments page, this
maps onto a method in a workflow. The workflow layer is an
implementation of the concepts needed for defining the scenarios. It
is written in terms of a application model; for a GUI application this
is typically written as a number of page or screen objects, each one
of which represents a single page in the application (or screen for a
thick client). This page model captures how the user interacts with
the pages and what the flow between them looks like. It hides the
details of the code that we are using to drive the application. For
example in a web application, the page objects have an interface
written in terms of what the user sees (buttons clicked, fields filled
in, links followed), whereas its implementation is written in terms of
HTML elements and XPath expressions. This implementation will
typically use a library designed for driving GUI apps, like Selenium,
Watir, White or Abbot.

Let's look at who has responsibility for these layers.

*[slide: bethany, terrence and scenario layer]

Bethany (the business analyst) and Terrence are responsible for the
contents of the scenario layer. Terrence can change the scenarios as
he needs to. If his changes are already supported by the workflow
layer then he's done. (Of course the tests will probably fail when
they're changed, but the developers only have to concern themselves
with the application code.)

*[slide: terrence, developer and workflow layer]

The implementation of the workflow layer in terms of the application
model is also Terrence's domain. He cares that the application model
is a faithful representation of how the application looks to a
user. But he probably won't write this stuff himself; he'll pair on
writing it with a developer (I think this is Delilah, they are hard to
tell apart). Between them they will ensure that the code accurately
models the application and that it is reasonably efficient and easy to
implement in the layer below.

*[slide: developers and application model]

After that it's developers all the way down. The developers own the
code in the application model and of course the application itself.

*[slide: terrence, developer and architecture]

So during the development of a story, Delilah first pairs with
Terrence to fill in the code under the scenario,

*[slide: developers and application model]

then she pairs with Dave to finish the test code.

[slide developers and application]

Finally they turn to the application code.

Notice that as well as using the scenario to drive the functionality
of the code that is being written, we are also using the test harness
that underlies it to drive the details of the UI. For example we are
writing XPath expressions in our page objects before we write the HTML
that they will be querying. This forces us to consider testability at
the application and UI level in the same way that TDD does at the
object level.

*[slide: driving everything]

Driving everything
------------------

I intend that the idea of driving development with scenarios should be
taken very seriously and should start right at the beginning. I think
that sketching out a candidate set of activities that the application
will support should be pretty much the first piece of analysis that is
carried out on a project.

*[slide: bethany, cesar, activities and scenarios]

This set of activities, and the scenarios that illustrate them, act as
a manifesto for the project. They enable the customers to clarify what
they want from the application and provide a vision for the
application which is shared amongst the team.

*[slide: activities, scenarios and team]

The scenarios are not *just* *tests*. Their identity, purpose and flow
should be familiar to everyone on the team; and the team, including
the customer, should evolve them together during the project. They are
the clearest, high-level, localized statement of what the application
*is*.

I want to take a slightly philosophical diversion now. I hope that
you'll bear with me. I find this approach invaluable for appreciating
the utility of scenarios (and incidentally for understanding all sorts
of things that can go wrong with projects). This analysis is the
second Main Idea that I would like to you take from this talk.

Any application has a notional, abstract form which captures what the
application *is*, it's essence. Unfortunately this essence isn't
written down anywhere where everyone can see it. Presumably it is this
essence that vast requirements documents are supposed to capture, but
of course they fail because, while they are long, they can't be
correct and they can't adapt as we learn things during
development. Instead of being able to point at the essence, it has to
be taken on trust.

*[slide: team and views]

Parvinder and Bethany and Terrence and Doreen and Delilah and Dave and
Cesar all have their own view of what this essence is, their own
mental models of the applications.

*[slide: team and single view]

I suspect that, if we could measure it, we would find that the extent
to which the members of a team share a view of this essence (and the
extent to which their view is clear and complete) is a strong
indicator of the success of a project.

We can't see the essence, but we can see *projections* of it into
various media. Each of these projections is incomplete and distorted,
because of the nature of the media into which it is made. When you
project a three-dimension object into two-dimensional space in order
to draw it, you make a decision about how to make the projection in
order to best show what you want. But which ever projection you chose
distorts the original object to some extent, misrepresents it and
leaves out some details.

The most obvious projection of an application's essence is the code;
this is the projection that is most obvious to Doreen and the other
developers. But while there may be a lot of it, the code is not
complete (it doesn't include the behaviour of the people who use the
software, or the environment that it runs in). And there maybe so much
of it that it isn't clear what the sum of the parts comes to.

*[slide: essence and projections]

All the team members have a greater or lesser familiarity with various
other projections, which include the documentation, the unit tests,
maybe even the marketing material. The complete set of implemented
stories is another projection.

*[slide: essence and story]

An individual story can be seen as a delta to the essence. A
well-written story steers clear of talking about the implementation;
in a team where the UI design work is incorporated into the
development it may even avoid too many details about the interface. So
what it talks about is the essence.

*[slide: projections and story]

And when any member of the team reads the story they naturally project
that essence into the medium that they are most familiar with. The UI
designer sees how the flow of the UI needs to change to accommodate
some new widget; Terrence imagines the edge cases and interactions
that need testing; the developers can see which modules in the code
need to change; the technical writer knows immediately which sections
of the documentation need to be updated.

And of course the scenarios are just one more projection. But they are
an interesting one. They are rather abstracted from the details of the
implementation. They are a projection into a rather rarified medium
which exists only to accommodate that project. And they are shared
amongst the entire team, written in a language that everyone can
understand. When a new story is played, everyone should agree on the
affect of it on the scenarios, rather than having their own private
view. So perhaps the scenarios are the closest thing we have to a
faithful projection of the essence (although of course we pay a price
in the paucity of detail).

*[slide: evolving scenarios]

The process of developing an application can be framed in terms of
elaboration of the application's scenarios. The identity of the
scenarios remains roughly constant during development, but their
content evolves. That evolution, and the particular scenarios that are
evolving, provide themes for the team during a project. "This
iteration we are going to fill in the details of the recurring
payments scenario." "This release focuses on improving the useability
of the admin scenarios."

Concerns
--------

I now want to talk about some of the forces that are at play and
decisions that need to be taken if you take a scenario-driven
approach.

*Collaboration*

*[slide: terrence and developers]

Earlier I described a close collaboration between testers and
developers, pairing to write the code behind the scenarios. I would
like to take that collaboration further and maintain it throughout the
development of a story. I talked about leaving some aspects of the
story's acceptance criteria to be tested by lower-level tests -- unit
tests, integration tests. As well as pairing on the scenarios,
developers and testers can pair on writing these tests, to give the
tester confidence that the tests do what they need to. And as the
story is implemented, piece by piece, the developers let the tester
know what has been checked in, what should be working correctly (and
that he can test) and what has not yet been implemented. This way, by
the time the developers are finished (or a very short while after) the
tester is finished with the story too.

*[slide: broken scenarios]

Another issue concerning team collaboration is what to do with the
scenarios that are affected by a story, while that story is under
development. Usually when we have updated the scenarios to reflect the
new story, they will fail. They probably won't be fixed until the
story is nearly finished. This leaves the team in the awkward position
of having test failing that they can't fix immediately. There will
probably be more than one scenario failing at once.

There are a number of possible approached to dealing with this (I
discount leaving them to fail and having a permanently broken
build). Of course the whole problem can be alleviated somewhat by
always concentrating first, when developing a story, on the mainline
functionality that is captured in the scenario and leaving till later
the edge cases and error handling that aren't reflected in a
scenario. I think this is a good idea anyway as it allows you to get
more immediate feedback on the story, before it is finished, from
testers and even from the business analyst and customer.

You can mark "in progress" scenarios in some way, with a tag
or by moving them to a different folders; the build is modified so
that failing "in progress" scenarios don't fail the whole
build. Having written the scenario at the start of a story's
development and seen it fail in the way you expect, you could
temporarily modify it to remove the failing assertion (for example by
commenting it out).

Or you could modify the scenario to assert that the expected failure
happens. Although I haven't yet had a chance to try this, this is the
solution that I prefer because it forces you to reverse the process at
the end of story development -- suddenly you will find the scenario
failing because the original assertion no longer fails; this approach
also ensures that other errors causing the scenario to fail won't
creep in because it is being temporarily ignored.

*Technical*

The remaining concerns are technical.

*[slide: drive points]

One major decision in any high-level test like these scenarios is how
much of the system do we test. Should we drive the application through
its user-interface, or should we call directly into the code? Should
we include other applications? Should we stub out the network layer,
or should we include external calls?

There are no firm rules here. My general guidance is to test as much
of the system as possible, with the following constraints. Your test
should not include any systems that you don't control (for example no
servers that are managed by another team); remove components that make
the tests run slowly (for example application servers that take
minutes to bring up and down); try to keep all of the components of
your test system controllable from code in your test fixtures. If you
are taking a component out of the tests, try to execute all of your
code, right up to the edges (for example if you are stubbing out a
system that you integrate with then write a simulator for it and
interact with it over a realistic transport).

*[slide: scenario]

Another decision that you have to make is what technology to use for
writing your scenarios. In the example I showed you I used Twist,
which is extremely well suited to this approach. Other possibilities
that take a similar approach (although without the tool support) are
FitNesse (where tests are defined in wiki pages) and Cucumber, a DSL
designed for BDD-style testing. I have most frequently used plain old
code with a standard unit testing framework, making an effort to
ensure that the code is readable to non-developers, but generally
requiring developers to make significant changes to it.

*[slide: architecture with fixtures]

The part of the code supporting the scenarios that often needs most
attention, design and rework is the fixture code. The main workflow of
the scenarios themselves drive the application as the user would, so
the code to support that is a fairly simple model of the application's
interface. But the fixture required to allow the scenarios to be run
is often extensive: systems need to be running, users need to be
provisioned, various state needs to be set up, authentication needs to
be handled.

It is almost always impractical to write this fixture code by driving
the application through its standard interface. Doing so may be
extremely slow and somewhat brittle as it introduces dependencies in
every test on the common bits of the UI required for that
setup. Instead this kind of fixture code should be allowed to "cheat"
in whatever way is necessary to keep it lightning fast and avoid
brittleness. (This is an other example of my first Main Idea: what
you leave out of the tests is crucial.) The description of the fixture
in the test is kept minimal and very declarative; the code underneath
needs to be smart enough to work out what needs doing and an efficient
way of doing it.

I prefer to avoid canned seed data to be used as fixtures, except
where that data is truly common (for example locale-specific data that
would appear in the running application). And it makes individual
scenarios clearer if *all* the fixture required for them to run is
defined within the scenario itself, rather that being shared amongst
scenarios (in the standard setup/teardown patter).

Twist Demo
----------

TODO segue

TODO script for demo

Outro
-----

TODO summary
